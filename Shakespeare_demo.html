<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Shakespeare_demo - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-fcef06c9d577e41ebb505301432e688dd91d6a17e81e78af5ff2941e3347d31c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-cd5c43c3112852621986874c2ed725a046fd195376b365fe5a7aa239b0c4291f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-6fb640835bd45a2e2095758663e237aefe80671acacc2e6377eec5ecccb9004b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-6fe5c682680814f73a99154776ccf32bc56a24fec802fd246b6940df4d10ae10","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-516f7020d7cbab1b7b30e4ac9867e5c6cbde59d627c4b88a4fbf95f2610b2dfe","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.37","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"1a3f72aa31f1f7de59924962c2180fb4b0de66a9","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2541163413208849,"name":"Shakespeare_demo","language":"scala","commands":[{"version":"CommandV1","origId":2541163413208851,"guid":"c96cee10-d2ac-4430-9199-ae0a5e9ee59d","subtype":"command","commandType":"auto","position":1.0,"command":"// Let's read some Shakespeare!\nval textFile = sc.textFile(\"/FileStore/tables/apxlvb6y1485689122743/input.txt\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">textFile: org.apache.spark.rdd.RDD[String] = /FileStore/tables/apxlvb6y1485689122743/input.txt MapPartitionsRDD[765] at textFile at &lt;console&gt;:33\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485699312723E12,"submitTime":1.485699312711E12,"finishTime":1.485699314184E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c740e3cf-1c97-44c3-be98-07a939fa739d"},{"version":"CommandV1","origId":2541163413208872,"guid":"12f60789-d325-44d6-bf0f-b79ad4b34ef2","subtype":"command","commandType":"auto","position":1.5,"command":"textFile.top(10)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res93: Array[String] = Array(youth with comeliness plucked all gaze his way, when, youth of the city?, yourself ready in your cabin for the mischance of, your worships: more of your conversation would, your worship's pleasure I shall do with this wicked caitiff?, your whores, sir, being members of my occupation,, your voices,, your tongues, and not a word more., your pleasures; at the least if you take it as a, your people,)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:44: error: value show is not a member of org.apache.spark.rdd.RDD[String]\n              textFile.show()\n                       ^\n</div>","error":null,"workflows":[],"startTime":1.485712056076E12,"submitTime":1.485712056072E12,"finishTime":1.48571205636E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bfe8683a-7db9-4a14-b982-0220b8913cbf"},{"version":"CommandV1","origId":2541163413208852,"guid":"659564a9-4d95-4588-9399-fd38d123f54d","subtype":"command","commandType":"auto","position":2.0,"command":"// A lot of this is from Scala Programming Guide but I add comments\n\n// IMPORT PHASE\n\n// Import what we need: RegexTokenizer, Tokenizer\n// SQL functions not needed for Databricks (?)\nimport org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\nimport org.apache.spark.sql.functions._\n\n// GET MY DATA\n\n// Added myself: convert RDD to DataFrame\n// The column is called \"sentence\"\n// Each observation has multiple words\n// Each observation might be empty (spaces, paragraphs)\nval sentenceDataFrame = textFile.toDF(\"sentence\")\n\n// TOKENIZE PREPARATION\n\n// The tokenizer takes my Shakespeare sentence column and output a column called \"words\", which is tokenized\n// Tokenizer: \"wood is wood\" => \"wood\" \"is\" \"wood\"\nval tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n\n// The regex tokenizer goes a bit further than the tokenizer by separating punctuations (matching non word characters, not only word characters)\n// Such as \"wood.\" is \"wood\" + \"\"\n// Instead of the regular tokenizer where we have \"wood.\" as \"wood\"\nval regexTokenizer = new RegexTokenizer()\n  .setInputCol(\"sentence\")\n  .setOutputCol(\"words\")\n  .setPattern(\"\\\\W\") // alternatively .setPattern(\"\\\\w+\").setGaps(false)\n\n// This apparently allows to count the number of tokens in a sentence\nval countTokens = udf { (words: Seq[String]) => words.length }\n\n// We can transform using the tokenizer\nval tokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(false)\n\n// Instead of using a tokenizer, we can use a regular expression tokenizer\nval regexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\")\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(false)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----------------------------------------------------------+-----------------------------------------------------------------------+------+\n|sentence                                                   |words                                                                  |tokens|\n+-----------------------------------------------------------+-----------------------------------------------------------------------+------+\n|First Citizen:                                             |[first, citizen:]                                                      |2     |\n|Before we proceed any further, hear me speak.              |[before, we, proceed, any, further,, hear, me, speak.]                 |8     |\n|                                                           |[]                                                                     |1     |\n|All:                                                       |[all:]                                                                 |1     |\n|Speak, speak.                                              |[speak,, speak.]                                                       |2     |\n|                                                           |[]                                                                     |1     |\n|First Citizen:                                             |[first, citizen:]                                                      |2     |\n|You are all resolved rather to die than to famish?         |[you, are, all, resolved, rather, to, die, than, to, famish?]          |10    |\n|                                                           |[]                                                                     |1     |\n|All:                                                       |[all:]                                                                 |1     |\n|Resolved. resolved.                                        |[resolved., resolved.]                                                 |2     |\n|                                                           |[]                                                                     |1     |\n|First Citizen:                                             |[first, citizen:]                                                      |2     |\n|First, you know Caius Marcius is chief enemy to the people.|[first,, you, know, caius, marcius, is, chief, enemy, to, the, people.]|11    |\n|                                                           |[]                                                                     |1     |\n|All:                                                       |[all:]                                                                 |1     |\n|We know't, we know't.                                      |[we, know't,, we, know't.]                                             |4     |\n|                                                           |[]                                                                     |1     |\n|First Citizen:                                             |[first, citizen:]                                                      |2     |\n|Let us kill him, and we'll have corn at our own price.     |[let, us, kill, him,, and, we'll, have, corn, at, our, own, price.]    |12    |\n+-----------------------------------------------------------+-----------------------------------------------------------------------+------+\nonly showing top 20 rows\n\n+-----------------------------------------------------------+---------------------------------------------------------------------+------+\n|sentence                                                   |words                                                                |tokens|\n+-----------------------------------------------------------+---------------------------------------------------------------------+------+\n|First Citizen:                                             |[first, citizen]                                                     |2     |\n|Before we proceed any further, hear me speak.              |[before, we, proceed, any, further, hear, me, speak]                 |8     |\n|                                                           |[]                                                                   |0     |\n|All:                                                       |[all]                                                                |1     |\n|Speak, speak.                                              |[speak, speak]                                                       |2     |\n|                                                           |[]                                                                   |0     |\n|First Citizen:                                             |[first, citizen]                                                     |2     |\n|You are all resolved rather to die than to famish?         |[you, are, all, resolved, rather, to, die, than, to, famish]         |10    |\n|                                                           |[]                                                                   |0     |\n|All:                                                       |[all]                                                                |1     |\n|Resolved. resolved.                                        |[resolved, resolved]                                                 |2     |\n|                                                           |[]                                                                   |0     |\n|First Citizen:                                             |[first, citizen]                                                     |2     |\n|First, you know Caius Marcius is chief enemy to the people.|[first, you, know, caius, marcius, is, chief, enemy, to, the, people]|11    |\n|                                                           |[]                                                                   |0     |\n|All:                                                       |[all]                                                                |1     |\n|We know't, we know't.                                      |[we, know, t, we, know, t]                                           |6     |\n|                                                           |[]                                                                   |0     |\n|First Citizen:                                             |[first, citizen]                                                     |2     |\n|Let us kill him, and we'll have corn at our own price.     |[let, us, kill, him, and, we, ll, have, corn, at, our, own, price]   |13    |\n+-----------------------------------------------------------+---------------------------------------------------------------------+------+\nonly showing top 20 rows\n\nimport org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\nimport org.apache.spark.sql.functions._\nsentenceDataFrame: org.apache.spark.sql.DataFrame = [sentence: string]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_39b94ce71598\nregexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_193b9fcc2255\ncountTokens: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,IntegerType,Some(List(ArrayType(StringType,true))))\ntokenized: org.apache.spark.sql.DataFrame = [sentence: string, words: array&lt;string&gt;]\nregexTokenized: org.apache.spark.sql.DataFrame = [sentence: string, words: array&lt;string&gt;]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485699373142E12,"submitTime":1.48569937313E12,"finishTime":1.485699374964E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7305d905-2a49-4669-b0ef-28625f6420d7"},{"version":"CommandV1","origId":2541163413208853,"guid":"f821f0c5-0de4-460c-b6ad-4b7f307e2e88","subtype":"command","commandType":"auto","position":3.0,"command":"// What if we want to remove stop words?\n\n// Import the stop words remover of Spark, works well for English but I think it's a disaster in other languages (?)\nimport org.apache.spark.ml.feature.StopWordsRemover\n\n// When we call the remover, we remove stop words:\n// - words = column to filter for stop words\n// - filtered = column to create which contains filtered words\nval remover = new StopWordsRemover()\n  .setInputCol(\"words\")\n  .setOutputCol(\"filtered\")\n\n// We create the clean corpus, we did not do stemming yet (I want to use Porter's stemmer but I do not know where to load it in Databricks)\nval corpus_clean = remover.transform(regexTokenized)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.ml.feature.StopWordsRemover\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_842b50933774\ncorpus_clean: org.apache.spark.sql.DataFrame = [sentence: string, words: array&lt;string&gt; ... 1 more field]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.lang.IllegalArgumentException: Field \"raw\" does not exist.","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:228)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:228)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:58)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:227)\n\tat org.apache.spark.ml.feature.StopWordsRemover.transformSchema(StopWordsRemover.scala:108)\n\tat org.apache.spark.ml.feature.StopWordsRemover.transform(StopWordsRemover.scala:88)</div>","workflows":[],"startTime":1.485699493476E12,"submitTime":1.485699493465E12,"finishTime":1.48569949357E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"37d18c0a-80cf-4cd3-88e9-20f68c57ea5c"},{"version":"CommandV1","origId":2541163413208856,"guid":"1597e336-f089-4da0-8b09-9ccf46fe79ce","subtype":"command","commandType":"auto","position":3.5,"command":"// Now we got a clean corpus (still not stemmed, unfortunately - where is Porter's Stemmer?)\n// We need to be able to pivot the data by word\n// Therefore, we must explode the filter word column containing one array per observation, then group against it.\nval counter = corpus_clean.withColumn(\"filtered\", explode(col(\"filtered\"))).groupBy(\"filtered\").agg(count(\"*\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">counter: org.apache.spark.sql.DataFrame = [filtered: string, count(1): bigint]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485700423558E12,"submitTime":1.485700423546E12,"finishTime":1.485700423649E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5d2c1cef-1332-4c55-b5ac-4cbfc0a4cdd6"},{"version":"CommandV1","origId":2541163413208861,"guid":"7bfae804-d197-4dca-9a86-0e6e81392d1e","subtype":"command","commandType":"auto","position":4.25,"command":"counter.printSchema","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- filtered: string (nullable = true)\n |-- count(1): long (nullable = false)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485700559595E12,"submitTime":1.48570055958E12,"finishTime":1.485700559658E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2c6e0fb4-fba9-4891-8dc8-d80b33732696"},{"version":"CommandV1","origId":2541163413208855,"guid":"9b38592e-81c9-4d8a-bf11-d985a89ad94e","subtype":"command","commandType":"auto","position":5.0,"command":"// We must rename columns.\n// Here we rename columns 1 by 1, I think there must be better ways to do that.\n// We also perform an invert sorting of the data by the count of word frequency.\nval counter_better = counter.withColumnRenamed(\"count(1)\", \"count\").withColumnRenamed(\"filtered\", \"word\").sort(desc(\"count\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">counter_better: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string, count: bigint]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.sql.AnalysisException: cannot resolve '`count`' given input columns: [filtered, count(1)];;","error":"<div class=\"ansiout\">'Sort ['count DESC], true\n+- Aggregate [filtered#7782], [filtered#7782, count(1) AS count(1)#7790L]\n   +- Project [sentence#5911, words#5931, filtered#7782]\n      +- Generate explode(filtered#6145), true, false, [filtered#7782]\n         +- Project [sentence#5911, words#5931, UDF(words#5931) AS filtered#6145]\n            +- Project [sentence#5911, UDF(sentence#5911) AS words#5931]\n               +- Project [value#5909 AS sentence#5911]\n                  +- LogicalRDD [value#5909]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:283)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:283)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$8.apply(QueryPlan.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:288)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:161)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:167)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:59)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2613)\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:2601)\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:878)</div>","workflows":[],"startTime":1.485700847629E12,"submitTime":1.485700847618E12,"finishTime":1.485700847707E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bf269946-3271-4144-9a16-879f1f1bd81d"},{"version":"CommandV1","origId":2541163413208860,"guid":"64b1c836-c68e-437a-992f-c016207b6fc0","subtype":"command","commandType":"auto","position":6.0,"command":"// I expect on the original corpus:\n// thou: 1896\n// thy: 1188\n// king: 1293\n// Why does not it match on this corpus? Probably due to some words being removed, we have less words than the original corpus.\ncounter_better.show(25)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+-----+\n|   word|count|\n+-------+-----+\n|   thou| 1421|\n|    thy| 1059|\n|   king|  925|\n|  shall|  849|\n|   thee|  762|\n|   lord|  711|\n|   good|  672|\n|   come|  624|\n|    sir|  597|\n|  would|  535|\n|    let|  528|\n|   well|  516|\n|   duke|  471|\n|    say|  459|\n|   hath|  454|\n|   love|  432|\n|    one|  427|\n|     go|  425|\n|    may|  409|\n|   make|  400|\n|     us|  399|\n|   upon|  399|\n|   like|  391|\n|richard|  388|\n|    yet|  387|\n+-------+-----+\nonly showing top 25 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:57: error: value takeOrdered is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n              counter_better.takeOrdered(10)(Ordering[Int].reverse.on(x=&gt;x._2))\n                             ^\n</div>","error":null,"workflows":[],"startTime":1.485703088161E12,"submitTime":1.485703088155E12,"finishTime":1.485703089391E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a25a9a71-74f2-4074-9a04-37cea852708c"},{"version":"CommandV1","origId":2541163413208863,"guid":"8f6efae7-452a-4e84-b4cc-fb58774f1218","subtype":"command","commandType":"auto","position":7.0,"command":"// We want to normalize the count values\n// MinMaxScaler idea failed, I think I need to go through Spark vectors to use it. See error of the cell below.\nimport org.apache.spark.ml.feature.MinMaxScaler\n\nval perc100_scaler = new MinMaxScaler()\n    .setInputCol(\"count\")\n    .setOutputCol(\"normalized_count\")\n    .setMax(100)\n    .setMin(0)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.ml.feature.MinMaxScaler\nperc100_scaler: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_d45daf2d31ea\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485701098125E12,"submitTime":1.48570109811E12,"finishTime":1.485701098209E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e13e1523-caf3-4125-aca9-48e3b25f1356"},{"version":"CommandV1","origId":2541163413208864,"guid":"fad1ae74-d8ca-46c0-977b-e416233507f4","subtype":"command","commandType":"auto","position":8.0,"command":"// Not working because not a Spark vector?\nperc100_scaler.fit(counter_better).transform(counter_better).show(25)","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.lang.IllegalArgumentException: requirement failed: Column count must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually LongType.","error":"<div class=\"ansiout\">\tat scala.Predef$.require(Predef.scala:233)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\n\tat org.apache.spark.ml.feature.MinMaxScalerParams$class.validateAndTransformSchema(MinMaxScaler.scala:67)\n\tat org.apache.spark.ml.feature.MinMaxScaler.validateAndTransformSchema(MinMaxScaler.scala:88)\n\tat org.apache.spark.ml.feature.MinMaxScaler.transformSchema(MinMaxScaler.scala:124)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:70)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:114)</div>","workflows":[],"startTime":1.485701135973E12,"submitTime":1.485701135973E12,"finishTime":1.485701136062E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"71334969-ce8a-4b12-bcc1-727b2b5315ae"},{"version":"CommandV1","origId":2541163413208865,"guid":"881db694-43d8-4032-8d20-b881eee95461","subtype":"command","commandType":"auto","position":9.0,"command":"// I do not know what is happening here (the printed values, not the command)\ncounter_better.groupBy(\"word\").max(\"count\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----------+----------+\n|       word|max(count)|\n+-----------+----------+\n|        art|       220|\n|      still|       127|\n|       hope|        98|\n|       earl|        28|\n|    embrace|        13|\n|     waters|        12|\n| lieutenant|         9|\n|      spoil|         7|\n|     shroud|         6|\n|    nourish|         5|\n| likelihood|         5|\n|     ransom|         5|\n|      crest|         4|\n|     biting|         4|\n|  solemnity|         4|\n|ingratitude|         3|\n|  arguments|         3|\n|      turks|         3|\n|     travel|         3|\n|     harder|         3|\n+-----------+----------+\nonly showing top 20 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:40: error: not found: value df\n              df.groupBy(&quot;word&quot;).max(&quot;count&quot;).show()\n              ^\n</div>","error":null,"workflows":[],"startTime":1.485701453168E12,"submitTime":1.485701453158E12,"finishTime":1.485701455689E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fc65a31d-fd54-4819-9bcb-82edf792dec7"},{"version":"CommandV1","origId":2541163413208866,"guid":"20e178f4-ad81-4726-8254-f62210c7c8a3","subtype":"command","commandType":"auto","position":10.0,"command":"// Get summary statistics of our counting\nval described_counter = counter_better.describe()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">described_counter: org.apache.spark.sql.DataFrame = [summary: string, count: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485701611223E12,"submitTime":1.485701611213E12,"finishTime":1.48570161254E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"62852ddc-2d5a-43cb-a5f6-5d97b4ce955f"},{"version":"CommandV1","origId":2541163413208867,"guid":"4acecb0d-91a7-4c14-9a24-d58bf00be848","subtype":"command","commandType":"auto","position":11.0,"command":"// Let's check the min/max and the total number of words (count = 11322)\ndescribed_counter.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+-----------------+\n|summary|            count|\n+-------+-----------------+\n|  count|            11322|\n|   mean|9.472619678502031|\n| stddev|38.17626335148655|\n|    min|                1|\n|    max|             1421|\n+-------+-----------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:60: error: value row is not a member of org.apache.spark.sql.DataFrame\n              described_counter.row(2)\n                                ^\n</div>","error":null,"workflows":[],"startTime":1.485701717359E12,"submitTime":1.485701717349E12,"finishTime":1.485701717429E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d00a3bb-a424-465e-a06b-6b31ad05c488"},{"version":"CommandV1","origId":2541163413208868,"guid":"5f1f3acd-5aa7-4114-8162-91ca789b9d9a","subtype":"command","commandType":"auto","position":12.0,"command":"// If we use collect we avoid Spark's laziness\n// Here, we pop the count column of the summary statistics table\nval described_vector = described_counter.select(\"count\").collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">described_vector: Array[org.apache.spark.sql.Row] = Array([11322], [9.472619678502031], [38.17626335148655], [1], [1421])\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:2: error: identifier expected but integer literal found.\n       described_counter.select(&quot;count&quot;).collect()[4, 5]\n                                                   ^\n</div>","error":null,"workflows":[],"startTime":1.485701951849E12,"submitTime":1.485701951839E12,"finishTime":1.485701951923E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"81957488-b2db-4304-8fea-9786d953bdfc"},{"version":"CommandV1","origId":2541163413208869,"guid":"2085e8cb-60ad-4f0c-9b4d-fa19c2564510","subtype":"command","commandType":"auto","position":13.0,"command":"// Now we can normalize data!\n// We create the \"norm_count\" column which is (Value - Min) / (Max - Min), where Min = 1 guaranteed.\n// Strictly speaking, we should use described_vector(3).getString(0).toInt for the minimum.\nval norm_count = counter_better.withColumn(\"norm_count\", (col(\"count\").cast(\"double\") - 1) / (described_vector(4).getString(0).toInt - 1))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">norm_count: org.apache.spark.sql.DataFrame = [word: string, count: bigint ... 1 more field]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:60: error: not found: value DoubleType\n       val norm_count = ((counter_better.col(&quot;count&quot;).cast(DoubleType) - 1) / (described_vector(4).getString(0).toInt - 1))\n                                                           ^\n</div>","error":null,"workflows":[],"startTime":1.485711915726E12,"submitTime":1.485711915717E12,"finishTime":1.485711915839E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b27f6138-fe87-49ab-ac1b-9a6e0ed531d9"},{"version":"CommandV1","origId":2541163413208870,"guid":"13a13716-5655-4442-99f9-2dec47513f91","subtype":"command","commandType":"auto","position":14.0,"command":"// And now we can print the normalized count.\n// The results are exactly what we expected.\n// Strangely, we get those exact 0.5 / 0.3 normalized counts, this sounds anecdotical luck.\nnorm_count.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----+-----+-------------------+\n| word|count|         norm_count|\n+-----+-----+-------------------+\n| thou| 1421|                1.0|\n|  thy| 1059| 0.7450704225352113|\n| king|  925| 0.6507042253521127|\n|shall|  849| 0.5971830985915493|\n| thee|  762| 0.5359154929577464|\n| lord|  711|                0.5|\n| good|  672| 0.4725352112676056|\n| come|  624| 0.4387323943661972|\n|  sir|  597| 0.4197183098591549|\n|would|  535|  0.376056338028169|\n|  let|  528| 0.3711267605633803|\n| well|  516| 0.3626760563380282|\n| duke|  471|0.33098591549295775|\n|  say|  459|0.32253521126760565|\n| hath|  454| 0.3190140845070423|\n| love|  432|0.30352112676056336|\n|  one|  427|                0.3|\n|   go|  425|0.29859154929577464|\n|  may|  409|0.28732394366197184|\n| make|  400|0.28098591549295776|\n+-----+-----+-------------------+\nonly showing top 20 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:64: error: value show is not a member of org.apache.spark.sql.Column\n              norm_count.show()\n                         ^\n</div>","error":null,"workflows":[],"startTime":1.485711917776E12,"submitTime":1.485711917767E12,"finishTime":1.485711918896E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0da0469d-473b-4b1e-8612-1f735dd752a6"},{"version":"CommandV1","origId":2541163413208873,"guid":"0f23233d-a448-4bbc-a1a4-e5ec8a67ec02","subtype":"command","commandType":"auto","position":14.5,"command":"// Convoluted way to get the value to an array.\n// Aren't there either ways to do that?\n// norm_count.select(\"count\").rdd.map(r => r(0)).collect()\n// Strangely, the function below outputs an ARRAY, which is obviously less convoluted than what we had\nnorm_count.select(\"count\").take(10).toList\n\n// This gets the normalized values\n// (norm_count.select(\"count\").rdd.map(r => r(0)).collect()).toList","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res120: List[org.apache.spark.sql.Row] = List([1421], [1059], [925], [849], [762], [711], [672], [624], [597], [535])\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:66: error: value rdd is not a member of Array[org.apache.spark.sql.Row]\n              norm_count.select(&quot;count&quot;).take(10).rdd.map(r =&gt; r(0)).collect()\n                                                  ^\n</div>","error":null,"workflows":[],"startTime":1.485716910889E12,"submitTime":1.485716910889E12,"finishTime":1.485716911829E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cd67bf61-b62b-4b66-8d5f-c5fde24179df"},{"version":"CommandV1","origId":2541163413208871,"guid":"de4caf53-b283-48c8-b578-93331bbcd400","subtype":"command","commandType":"auto","position":15.0,"command":"// We can try to make a basic bar chart!\n// THIS DOES NOT WORK IN DATABRICKS\n// :(\n\n// First we import what we need from Plotly (d3js behind the hood)\nimport co.theasi.plotly._\nimport co.theasi.plotly.element._\n\n// Let's get our data in an appropriate form\nval my_data = Seq(\n  Bar(\n    norm_count.select(\"word\").take(10).toList,\n    norm_count.select(\"count\").take(10).toList\n  )\n)\n\nPlotly.plot(\"whatever\", my_data)","commandVersion":0,"state":"error","results":null,"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:66: error: not found: value co\n              import co.theasi.plotly._\n                     ^\n&lt;console&gt;:67: error: not found: value co\n              import co.theasi.plotly.element._\n                     ^\n&lt;console&gt;:71: error: not found: value Bar\n                Bar(\n                ^\n&lt;console&gt;:79: error: not found: value Plotly\n              Plotly.plot(&quot;whatever&quot;, my_data)\n              ^\n</div>","error":null,"workflows":[],"startTime":1.485717137414E12,"submitTime":1.485717137414E12,"finishTime":1.48571713745E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a57f743b-e692-497f-8677-6c0da5a9839f"},{"version":"CommandV1","origId":2541163413208876,"guid":"882b8a93-efae-41ef-89b6-a8171510a02e","subtype":"command","commandType":"auto","position":16.0,"command":"// Let's try to do something with the table\ncounter_better.createOrReplaceTempView(\"counter_better\")\nsqlContext.sql(\"SELECT word, count FROM counter_better LIMIT 10\").collect().foreach(println)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[thou,1421]\n[thy,1059]\n[king,925]\n[shall,849]\n[thee,762]\n[lord,711]\n[good,672]\n[come,624]\n[sir,597]\n[would,535]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.sql.AnalysisException: Table or view not found: counter_better; line 1 pos 24","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:457)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:461)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:451)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:583)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)</div>","workflows":[],"startTime":1.485718073442E12,"submitTime":1.48571807343E12,"finishTime":1.485718074433E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dbf1f220-b404-4c18-a747-030609809178"},{"version":"CommandV1","origId":2541163413208877,"guid":"d2be078c-89c5-4753-86fe-1af1bfda82a9","subtype":"command","commandType":"auto","position":17.0,"command":"// It is not in Databricks too?!\nimport smile.plot.BarPlot\nBarPlot.plot(\"\", norm_count.select(\"count\").take(10).toList, norm_count.select(\"word\").take(10).toList)","commandVersion":0,"state":"error","results":null,"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:62: error: not found: value smile\n              import smile.plot.BarPlot\n                     ^\n&lt;console&gt;:65: error: not found: value BarPlot\n              BarPlot.plot(&quot;&quot;, norm_count.select(&quot;count&quot;).take(10).toList, norm_count.select(&quot;word&quot;).take(10).toList)\n              ^\n</div>","error":null,"workflows":[],"startTime":1.485718139668E12,"submitTime":1.485718139668E12,"finishTime":1.485718139698E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7cbdbacc-5cc7-47fe-95b4-c1571787d530"},{"version":"CommandV1","origId":2541163413208878,"guid":"10e27c65-6329-4552-8d0c-aa322c266962","subtype":"command","commandType":"auto","position":18.0,"command":"// Where is my file???\nnorm_count.write.format(\"com.databricks.spark.csv\").\n                         option(\"header\",\"true\").\n                         save(\"out-csv\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:64: error: not found: value writeCsv\n              writeCsv(norm_count, new File(&quot;norm_count.csv&quot;))\n              ^\n</div>","error":null,"workflows":[],"startTime":1.485718579802E12,"submitTime":1.485718579797E12,"finishTime":1.485718584322E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"91f3f878-1d2b-45d7-9ffc-6e6e8bb47019"},{"version":"CommandV1","origId":2541163413208879,"guid":"097e3ed2-3980-4e2f-a00b-ae268ca3944c","subtype":"command","commandType":"auto","position":19.0,"command":"// I recommend the following resources if the objective is to start learning Scala:\n// Coursera: https://www.coursera.org/learn/progfun1/ and all subsequent parts, it should take at least 200 hours if we include good practice!\n// Big Data University: https://bigdatauniversity.com/learn/scala/ but the Spark part is too fast, the Data Science part is very quick\n// Regular Expressions in Scala (and other stuff): https://www.tutorialspoint.com/scala/scala_regular_expressions.htm\n// Scala for Data Science: https://github.com/pbugnion/s4ds and it seems the best choice with many code examples!\n// ^ chapter 10+11 (Spark) & chapter 12 (MLlib) chapter 13 (Web API) & chapter 14 (d3js) are best, although for pure supervised ML (let's say gradient boosted trees) it might be better to offload using C++ programs... (poor performance of MLlib both in performance, speed, and RAM usage) but for that we need to pop the DataFrame using collect which might explode in memory.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f7610276-20c9-464c-a0ec-5c6cbe9a5c4d"},{"version":"CommandV1","origId":2541163413208880,"guid":"0e0639f6-f6a6-4247-b448-8c2cc9b979c3","subtype":"command","commandType":"auto","position":20.0,"command":"// Can we do all in one shot with a comprehensive class?\n// \n// Let's introduce a new class: TheParser\n// It can fetch data according to the URL you provide (URI not supported)\n// It uses Spark DataFrame behind the hood to store data (in memory... but should be very small)\n// It uses a regular expression tokenizer, removing punctuations, and removing any words smaller or equal than 3 in length\n// .URL = URL input\n// .dataURL = URL source\n// .textFile = filtered source (array)\n// .sentenceDataFrame = DataFrame filtered source\n// .regexTokenized = transformed DataFrame\n// .corpus_clean = cleaned corpus (no stop words)\n// .counter = count words\n// .counter_better = count words but in descending order\n// .described_counter = sum, mean, standard deviation, minimum, maximum of count\n// .show(int) = print the int-first most frequent words, will not crash if int>number of different words\n// .describe() = print .described_counter\n// \n// test case: sc.parallelize(scala.io.Source.fromURL(\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\").mkString.replaceAll(\"[0-9]\", \"\").split(\"\\\\n\").filter(_ != \"\")).toDF(\"sentence\")\n// test case: (new TheParser(\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\")).show(25)\n// \n// TO DO: add error handling, and perhaps more features (word cloud? but Google could not find any word cloud package for Scala...)\n// error handling, one can do:\n// Try {scala.io.Source.fromURL(\"http://www.google.com\")}\n// Try {scala.io.Source.fromURL(\"garbage\")}\n\nclass TheParser (url: String) {\n  \n  private val regexTokenizer = new RegexTokenizer()\n  .setInputCol(\"sentence\")\n  .setOutputCol(\"words\")\n  .setPattern(\"\\\\W\")\n  .setMinTokenLength(3)\n  \n  private val remover = new StopWordsRemover()\n  .setInputCol(\"words\")\n  .setOutputCol(\"filtered\")\n  \n  val URL = url\n  def dataURL = scala.io.Source.fromURL(url)\n  val textFile = dataURL.mkString.replaceAll(\"[0-9]\", \"\").split(\"\\\\n\").filter(_ != \"\")\n  val sentenceDataFrame = sc.parallelize(textFile).toDF(\"sentence\")\n  val regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n  val corpus_clean = remover.transform(regexTokenized)\n  val counter = corpus_clean.withColumn(\"filtered\", explode(col(\"filtered\"))).groupBy(\"filtered\").agg(count(\"*\"))\n  val counter_better = counter.withColumnRenamed(\"count(1)\", \"count\").withColumnRenamed(\"filtered\", \"word\").sort(desc(\"count\"))\n  val described_counter = counter_better.describe()\n  \n  def show = (many:Integer) => {counter_better.show(many)}\n  def describe = () => described_counter.show()\n  \n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">defined class TheParser\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:93: error: not found: value dataFile\n         val textFile = dataFile.mkString.replaceAll(&quot;[0-9]&quot;, &quot;&quot;).split(&quot;\\\\n&quot;).filter(_ != &quot;&quot;)\n                        ^\n&lt;console&gt;:86: error: type mismatch;\n found   : Unit\n required: scala.util.Try[List[String]]\n         }\n         ^\n</div>","error":null,"workflows":[],"startTime":1.485724579316E12,"submitTime":1.485724579303E12,"finishTime":1.485724579582E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"36a68627-f0e5-456f-89ca-4c3fe3fb7a79"},{"version":"CommandV1","origId":2541163413208881,"guid":"b65390de-b480-4816-ad89-d849ba7fd3ee","subtype":"command","commandType":"auto","position":21.0,"command":"// Hurrah!\nval shakespeare = new TheParser(\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\")\nshakespeare.describe()\nshakespeare.show(25)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+-----------------+\n|summary|            count|\n+-------+-----------------+\n|  count|            11272|\n|   mean|9.317157558552164|\n| stddev|37.72247014640279|\n|    min|                1|\n|    max|             1421|\n+-------+-----------------+\n\n+-------+-----+\n|   word|count|\n+-------+-----+\n|   thou| 1421|\n|    thy| 1059|\n|   king|  925|\n|  shall|  849|\n|   thee|  762|\n|   lord|  711|\n|   good|  672|\n|   come|  624|\n|    sir|  597|\n|  would|  535|\n|    let|  528|\n|   well|  516|\n|   duke|  471|\n|    say|  459|\n|   hath|  454|\n|   love|  432|\n|    one|  427|\n|    may|  409|\n|   make|  400|\n|   upon|  399|\n|   like|  391|\n|richard|  388|\n|    yet|  387|\n|    man|  381|\n|  queen|  380|\n+-------+-----+\nonly showing top 25 rows\n\nshakespeare: TheParser = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TheParser@59dd9b7b\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:62: error: Unit does not take parameters\n              shakespeare.describe()\n                                  ^\n</div>","error":null,"workflows":[],"startTime":1.485724583064E12,"submitTime":1.485724583054E12,"finishTime":1.485724585941E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f61048ab-c1af-4d51-bd3b-3ab9b95ff0be"},{"version":"CommandV1","origId":2541163413208882,"guid":"476034a9-b644-45db-8498-57f9ec70d298","subtype":"command","commandType":"auto","position":22.0,"command":"// If we did not remove the numbers, we would have had numbers taking the lead!\nval scala_ds = new TheParser(\"https://github.com/pbugnion/s4ds\")\nscala_ds.describe()\nscala_ds.show(25)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+-----------------+\n|summary|            count|\n+-------+-----------------+\n|  count|              722|\n|   mean|8.044321329639889|\n| stddev|23.10683688527083|\n|    min|                1|\n|    max|              404|\n+-------+-----------------+\n\n+----------+-----+\n|      word|count|\n+----------+-----+\n|     class|  404|\n|      span|  211|\n|      href|  128|\n|       div|  120|\n|       css|  112|\n|      data|  111|\n|  truncate|  105|\n|       svg|   93|\n|   octicon|   92|\n|  pbugnion|   91|\n|       sds|   90|\n|   content|   79|\n|      aria|   78|\n|    github|   78|\n|     width|   73|\n|      true|   71|\n|    height|   64|\n|       com|   64|\n|     https|   63|\n|      chap|   61|\n|    target|   58|\n|      item|   54|\n|      meta|   54|\n|navigation|   53|\n|      menu|   49|\n+----------+-----+\nonly showing top 25 rows\n\nscala_ds: TheParser = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TheParser@635d1d8f\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.io.IOException: Server returned HTTP response code: 403 for URL: https://medium.com/m/global-identity?redirectUrl=https://uxdesign.cc/lets-talk-about-design-portfolios-8a56ce1e9a8e","error":"<div class=\"ansiout\">\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1876)\n\tat sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1466)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1464)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1463)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254)\n\tat java.net.URL.openStream(URL.java:1045)\n\tat scala.io.Source$.fromURL(Source.scala:140)\n\tat scala.io.Source$.fromURL(Source.scala:130)\n\tat Notebook$TheParser.&lt;init&gt;(&lt;console&gt;:59)</div>","workflows":[],"startTime":1.485724590296E12,"submitTime":1.485724590284E12,"finishTime":1.485724592397E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dcc9d095-84c0-4d2b-b829-d2f53623b34b"},{"version":"CommandV1","origId":2541163413208885,"guid":"e15203fd-9e58-41d7-9094-b7e696788af4","subtype":"command","commandType":"auto","position":23.0,"command":"val scala_ds = new TheParser(\"http://www.perdu.com/\")\nscala_ds.describe()\nscala_ds.show(25)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+------------------+\n|summary|             count|\n+-------+------------------+\n|  count|                17|\n|   mean|1.5294117647058822|\n| stddev|0.6242642728467978|\n|    min|                 1|\n|    max|                 3|\n+-------+------------------+\n\n+--------+-----+\n|    word|count|\n+--------+-----+\n|    vous|    3|\n|  strong|    2|\n|    html|    2|\n|   title|    2|\n|   perdu|    2|\n|    head|    2|\n|    body|    2|\n|     pre|    2|\n|   aider|    1|\n|     ici|    1|\n|     sur|    1|\n|    etes|    1|\n|internet|    1|\n|     pas|    1|\n| panique|    1|\n|     tes|    1|\n|   ecirc|    1|\n+--------+-----+\n\nscala_ds: TheParser = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TheParser@4e684a7c\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485724601885E12,"submitTime":1.485724601874E12,"finishTime":1.485724603157E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"67057694-ecd8-4681-892e-ae19f7544d72"},{"version":"CommandV1","origId":2541163413208886,"guid":"fdaff3f0-0604-476c-888c-69a3e7c440e8","subtype":"command","commandType":"auto","position":24.0,"command":"val scala_ds = new TheParser(\"http://symbolics.com/\")\nscala_ds.describe()\nscala_ds.show(25)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------+------------------+\n|summary|             count|\n+-------+------------------+\n|  count|               303|\n|   mean|3.5214521452145213|\n| stddev| 5.998236650666954|\n|    min|                 1|\n|    max|                63|\n+-------+------------------+\n\n+---------+-----+\n|     word|count|\n+---------+-----+\n|    class|   63|\n|      div|   38|\n|      com|   37|\n|     href|   27|\n|   script|   27|\n|symbolics|   25|\n|      src|   21|\n|     text|   20|\n|     http|   20|\n|   domain|   20|\n|     span|   18|\n|     type|   14|\n|    offer|   14|\n|    title|   13|\n|      img|   11|\n| function|   11|\n|    logos|   11|\n|  section|   11|\n|     gray|   10|\n|      css|   10|\n|     name|   10|\n|  article|   10|\n|      pos|    9|\n|    false|    9|\n|  contact|    9|\n+---------+-----+\nonly showing top 25 rows\n\nscala_ds: TheParser = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TheParser@44133cb4\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.485724607237E12,"submitTime":1.485724607226E12,"finishTime":1.48572460868E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d0e3fe51-b20f-427d-9913-cfb8ff33d797"},{"version":"CommandV1","origId":2541163413208887,"guid":"88cee0f0-7744-45ab-9d7f-a5fcda649931","subtype":"command","commandType":"auto","position":25.0,"command":"// Currently, does not resist garbage inputs which errors out immediately. Still a TO-DO\nval scala_ds = new TheParser(\"garbage\")\nscala_ds.describe()\nscala_ds.show(25)","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.net.MalformedURLException: no protocol: garbage","error":"<div class=\"ansiout\">\tat java.net.URL.&lt;init&gt;(URL.java:593)\n\tat java.net.URL.&lt;init&gt;(URL.java:490)\n\tat java.net.URL.&lt;init&gt;(URL.java:439)\n\tat scala.io.Source$.fromURL(Source.scala:130)\n\tat Notebook$TheParser.dataURL(&lt;console&gt;:98)\n\tat Notebook$TheParser.&lt;init&gt;(&lt;console&gt;:99)</div>","workflows":[],"startTime":1.485724612025E12,"submitTime":1.485724612025E12,"finishTime":1.485724612134E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2f307a29-ccc4-4d8b-9aad-c9931dca40ff"},{"version":"CommandV1","origId":2541163413208888,"guid":"15d1c170-55b7-42f1-b571-06942564000d","subtype":"command","commandType":"auto","position":26.0,"command":"// Let's get some information about memory usage\n// Because it is good to have something working, but is it not heavy?\n// After spamming so many bad commands and loading so many overwritten content (150 according to logs?), we used only 459MB.\n\n// Import in namespace what we need\nimport runtime.{totalMemory, freeMemory, maxMemory}\n\nval runtime = Runtime.getRuntime\nprintln(\"Used Memory:   \" + (runtime.totalMemory - runtime.freeMemory) / 1048576 + \"MB\")\nprintln(\"Free Memory:   \" + runtime.freeMemory / 1048576 + \"MB\")\nprintln(\"Total Memory:  \" + runtime.totalMemory / 1048576 + \"MB\")\nprintln(\"Max Memory:    \" + runtime.maxMemory / 1048576 + \"MB\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Used Memory:   459MB\nFree Memory:   3209MB\nTotal Memory:  3668MB\nMax Memory:    3668MB\nimport runtime.{totalMemory, freeMemory, maxMemory}\nruntime: Runtime = java.lang.Runtime@19f39d2f\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:51: error: not found: value logger\n              logger.log(&quot;Max Memory:    &quot; + runtime.maxMemory / 1048576 + &quot;MB&quot;)\n              ^\n&lt;console&gt;:46: error: not found: value logger\n              logger.log(&quot;Used Memory:   &quot; + (runtime.totalMemory - runtime.freeMemory) / 1048576 + &quot;MB&quot;)\n              ^\n&lt;console&gt;:47: error: not found: value logger\n              logger.log(&quot;Free Memory:   &quot; + runtime.freeMemory / 1048576 + &quot;MB&quot;)\n              ^\n&lt;console&gt;:48: error: not found: value logger\n              logger.log(&quot;Total Memory:  &quot; + runtime.totalMemory / 1048576 + &quot;MB&quot;)\n              ^\n</div>","error":null,"workflows":[],"startTime":1.485725862142E12,"submitTime":1.485725862133E12,"finishTime":1.485725862228E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d22b81a-fb8a-4192-9fa4-57f3f9ab9e93"}],"dashboards":[],"guid":"10ab0683-231d-40a5-98e8-87e3216f639b","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
